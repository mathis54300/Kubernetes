# üß© DaemonSet : un pod par n≈ìud

## 1. Introduction

Un **DaemonSet** garantit qu‚Äôun pod sp√©cifique s‚Äôex√©cute **sur chaque n≈ìud** (ou sur un sous‚Äëensemble cibl√©) d‚Äôun cluster. √Ä l‚Äôarriv√©e d‚Äôun nouveau n≈ìud, Kubernetes y planifie automatiquement un nouveau pod ; quand un n≈ìud quitte le cluster, le pod correspondant dispara√Æt.

Cas d‚Äôusage typiques‚Äâ:

- Collecte de logs (Fluent Bit, Promtail)
- Monitoring syst√®me (Node Exporter, cadvisor)
- Agents r√©seau/CNI, storage, s√©curit√© (Cilium, Calico, Falco)
- Side‚Äëservices ‚Äúnode‚Äëlocal‚Äù (DNS cache, time sync, etc.)

---

## 2. DaemonSet vs Deployment

- Objectif‚Äâ: DaemonSet = 1 pod par n≈ìud ; Deployment = N r√©plicas o√π que ce soit
- √âchelle‚Äâ: DaemonSet s‚Äôaligne sur le nombre de n≈ìuds ; Deployment sur un `replicas`
- Arriv√©e/Sortie de n≈ìud‚Äâ: DaemonSet cr√©e/supprime automatiquement le pod correspondant
- Mises √† jour‚Äâ: contr√¥l√©es par `updateStrategy` (RollingUpdate/OnDelete) au niveau du DaemonSet

---

## 3. Exemple minimal

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 10%
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      serviceAccountName: node-exporter
      containers:
      - name: node-exporter
        image: quay.io/prometheus/node-exporter:v1.8.1
        args: ["--path.rootfs=/host"]
        ports:
        - name: metrics
          containerPort: 9100
          hostPort: 9100
        volumeMounts:
        - name: rootfs
          mountPath: /host
          readOnly: true
      volumes:
      - name: rootfs
        hostPath:
          path: /
          type: Directory
```

‚Üí Expose les m√©triques sur chaque n≈ìud via `hostPort:9100`.

---

## 4. Cibler les n≈ìuds

Vous pouvez restreindre o√π les pods de DaemonSet s‚Äôex√©cutent‚Äâ:

- `nodeSelector`‚Äâ: filtrage simple par labels
- `affinity.nodeAffinity`‚Äâ: expressions avanc√©es
- `tolerations`‚Äâ: autoriser le scheduling sur des n≈ìuds ‚Äútaint√©s‚Äù (ex. control‚Äëplane)

Exemple‚Äâ: Linux uniquement + pr√©sence d‚Äôun label personnalis√©‚Äâ:

```yaml
spec:
  template:
    spec:
      nodeSelector:
        kubernetes.io/os: linux
        node-role: workload
```

Affinit√© avanc√©e‚Äâ:

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/arch
                operator: In
                values: [amd64, arm64]
```

Tolerations pour ex√©cuter aussi sur les n≈ìuds control‚Äëplane‚Äâ:

```yaml
spec:
  template:
    spec:
      tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
```

---

## 5. Acc√®s au n≈ìud : hostPath, hostNetwork, PID/IPC

Les agents ‚Äúnode‚Äëlocal‚Äù doivent souvent lire des fichiers ou √©couter un port sur le n≈ìud‚Äâ:

- `hostPath`‚Äâ: monte un dossier du n≈ìud dans le conteneur (ex. `/var/log`) ; attention aux permissions
- `hostNetwork: true`‚Äâ: le pod partage le r√©seau du n≈ìud (√©vite `hostPort`, mais isole moins)
- `hostPID: true` / `hostIPC: true`‚Äâ: acc√®s aux namespaces PID/IPC du n≈ìud (√† limiter)

Exemple Promtail (logs)‚Äâ:

```yaml
spec:
  template:
    spec:
      serviceAccountName: promtail
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: dockerlogs
        hostPath:
          path: /var/lib/docker/containers
      containers:
      - name: promtail
        image: grafana/promtail:2.9.3
        volumeMounts:
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: dockerlogs
          mountPath: /var/lib/docker/containers
          readOnly: true
```

---

## 6. Exposer un DaemonSet

Deux options courantes‚Äâ:

- `hostPort` (un port par n≈ìud)‚Äâ: pratique pour le scraping direct par IP de n≈ìud
- Service + endpoints (ClusterIP/Headless)‚Äâ: load‚Äëbalancing interne classique

Exemple Service pour Node Exporter‚Äâ:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: node-exporter
  namespace: monitoring
  labels:
    app: node-exporter
spec:
  clusterIP: None  # headless pour exposer chaque endpoint
  selector:
    app: node-exporter
  ports:
  - name: metrics
    port: 9100
    targetPort: metrics
```

‚Üí Scraper chaque endpoint (un par n≈ìud) via le service headless.

---

## 7. Strat√©gies de mise √† jour

- `RollingUpdate` (par d√©faut) avec `maxUnavailable` (valeur absolue ou %) pour limiter l‚Äôindisponibilit√© simultan√©e
- `OnDelete`‚Äâ: aucun remplacement automatique ; il faut supprimer les pods manuellement pour d√©clencher la mise √† jour

Exemple‚Äâ:

```yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
```

---

## 8. Op√©rations et d√©pannage

Commandes utiles‚Äâ:

- `kubectl get ds -A`
- `kubectl describe ds/<name> -n <ns>`
- `kubectl get pods -l app=node-exporter -o wide -n monitoring`
- `kubectl rollout status ds/<name> -n <ns>`
- `kubectl rollout restart ds/<name> -n <ns>`

Points d‚Äôattention‚Äâ:

- D√©finir `resources` (requests/limits) pour ne pas perturber le n≈ìud
- `priorityClassName` √©lev√© pour les agents critiques (ex. `system-node-critical`)
- V√©rifier taints/labels lors d‚Äôun ‚Äúpod manquant‚Äù sur un n≈ìud
- Sur clusters h√©t√©rog√®nes (Windows/Linux/arch), filtrer explicitement

---

## 9. Bonnes pratiques

- Minimiser les privil√®ges‚Äâ: `securityContext`, capacit√©s Linux limit√©es
- √âviter `hostNetwork`/`hostPID` si non n√©cessaires
- G√©rer la compatibilit√© multi‚ÄëOS/arch via labels
- Versionner et tester la strat√©gie d‚Äôupdate (canary par n≈ìud)
- Exporter des m√©triques et des logs pour l‚Äôobservabilit√© des agents

---

## 10. R√©sum√©

Les DaemonSets sont essentiels pour d√©ployer des agents ‚Äúpar n≈ìud‚Äù. Avec un ciblage pr√©cis (labels, affinit√©, tolerations), des volumes `hostPath` contr√¥l√©s et une strat√©gie d‚Äôupdate adapt√©e, vous obtenez des d√©ploiements robustes, observables et s√ªrs, align√©s sur la topologie r√©elle de votre cluster.

