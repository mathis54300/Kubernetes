# üåê Le r√©seau dans Kubernetes


## 1. Introduction

Le r√©seau Kubernetes est souvent per√ßu comme complexe ‚Äî et √† juste titre.
C‚Äôest l‚Äôun des piliers fondamentaux du cluster, mais aussi l‚Äôun des plus techniques.

Le r√¥le du **r√©seau Kubernetes** est de permettre :

* la communication entre **pods**,
* la communication entre **services**,
* et l‚Äôacc√®s des utilisateurs externes aux applications.

Tout cela dans un environnement **dynamique et √©ph√©m√®re**, o√π les pods apparaissent, disparaissent et changent de n≈ìud sans arr√™t.
Kubernetes doit donc garantir un **r√©seau stable, coh√©rent et transparent**, quel que soit l‚Äô√©tat du cluster.

---

## 2. Le mod√®le r√©seau Kubernetes

Kubernetes adopte un mod√®le r√©seau **plat et unifi√©**, fond√© sur trois grands principes :

1. **Chaque pod a sa propre IP**.
   ‚Üí Pas besoin de NAT entre pods.
2. **Tous les pods peuvent communiquer entre eux, sans NAT**.
   ‚Üí Peu importe le n≈ìud sur lequel ils tournent.
3. **Les pods peuvent communiquer avec les services du cluster.**

Cette philosophie vise la **simplicit√© applicative** : les d√©veloppeurs n‚Äôont plus √† g√©rer les adresses, NAT ou ports ; chaque service est accessible directement par son nom ou son IP interne.

---

## 3. Les composants r√©seau principaux

Le r√©seau Kubernetes repose sur plusieurs couches logicielles, chacune ayant un r√¥le bien pr√©cis :

| Composant                                      | R√¥le                                                              |
| ---------------------------------------------- | ----------------------------------------------------------------- |
| **Pod Network (CNI)**                          | G√®re l‚Äôattribution des adresses IP et la connectivit√© entre pods  |
| **Service Network**                            | Abstraction pour exposer les applications et √©quilibrer le trafic |
| **CoreDNS**                                    | Fournit la r√©solution de noms interne                             |
| **kube-proxy**                                 | Configure le routage du trafic et l‚Äô√©quilibrage de charge (Juste entre les services et les pods)        |
| **CNI Plugin (Cilium, Calico, Flannel, etc.)** | Impl√©mente la couche r√©seau r√©elle (routage, s√©curit√©, politique) |

---

## üß© 3.1 Le r√©seau des pods (CNI)

Le r√©seau des pods est g√©r√© par une **interface standardis√©e appel√©e CNI** (Container Network Interface).
Le CNI d√©finit comment connecter un conteneur au r√©seau et comment nettoyer cette connexion √† sa suppression.

### üîß R√¥le du plugin CNI

Chaque plugin CNI :

* assigne une **adresse IP** √† chaque pod,
* configure les routes sur le n≈ìud,
* et s‚Äôassure que les pods peuvent communiquer entre n≈ìuds.

Les plugins populaires :

| Plugin        | Particularit√©s                                                     |
| ------------- | ------------------------------------------------------------------ |
| **Cilium**    | Bas√© sur eBPF, tr√®s performant, observabilit√© et s√©curit√© int√©gr√©e |
| **Calico**    | Routage IP natif, support des NetworkPolicies avanc√©es             |
| **Flannel**   | Simple √† d√©ployer, bon pour les petits clusters                    |

> üí° En pratique : Le plugin d√©termine la performance, la s√©curit√© et les capacit√©s r√©seau du cluster.

---

## ‚öôÔ∏è 3.2 kube-proxy : le gestionnaire de trafic interne

Le **kube-proxy** fonctionne sur chaque n≈ìud.
Il surveille (`watch`) le `kube-apiserver` pour d√©tecter les changements dans les **Services** et **Endpoints**.

Lorsqu‚Äôun service est cr√©√© ou qu‚Äôun pod devient disponible :

* le kube-proxy met √† jour les r√®gles r√©seau locales (`iptables` ou `ipvs`),
* il cr√©e un **√©quilibrage de charge local** entre les pods correspondant √† ce service.

### üîß Modes de fonctionnement :

* **iptables** : par d√©faut, utilise des r√®gles de pare-feu Linux ; simple et fiable.
* **ipvs** : plus performant, utile pour les gros clusters.
* **userspace** : mode historique, peu utilis√© aujourd‚Äôhui.

> üß† kube-proxy ne route pas lui-m√™me les paquets : il configure le syst√®me d‚Äôexploitation pour le faire.

---

## üîó 3.3 Les Services : abstraction r√©seau

Un **Service** Kubernetes est une ressource qui expose un ensemble de pods comme une seule entit√© logique, stable et accessible.
M√™me si les pods changent, le Service garde la m√™me adresse IP.

### Types de services :

| Type             | Description                                              | Usage                 |
| ---------------- | -------------------------------------------------------- | --------------------- |
| **ClusterIP**    | Expose le service uniquement √† l‚Äôint√©rieur du cluster    | Communication interne |
| **NodePort**     | Expose le service sur un port du n≈ìud                    | Acc√®s externe simple  |
| **LoadBalancer** | D√©l√®gue l‚Äô√©quilibrage √† un load balancer externe (Cloud) | Production, cloud     |
| **ExternalName** | Redirige vers un nom DNS externe                         | Int√©grations externes |

Exemple :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp
spec:
  type: ClusterIP
  selector:
    app: webapp
  ports:
  - port: 80
    targetPort: 8080
```

‚Üí Le Service `webapp` redirige vers les pods ayant le label `app=webapp`.

---

## üß† 3.4 CoreDNS : la r√©solution de noms interne

**CoreDNS** est un d√©ploiement interne (dans `kube-system`) qui fournit le **DNS du cluster**.
Il permet de contacter un service ou un pod via un nom logique plut√¥t qu‚Äôune IP.

Exemple :

```
http://backend.default.svc.cluster.local
```

* `backend` ‚Üí nom du service
* `default` ‚Üí namespace
* `svc` ‚Üí indique qu‚Äôil s‚Äôagit d‚Äôun service
* `cluster.local` ‚Üí domaine du cluster

CoreDNS interroge l‚ÄôAPI Server pour conna√Ætre les Services disponibles et met √† jour automatiquement ses enregistrements DNS.

> üí¨ Sans CoreDNS, la d√©couverte automatique des services ne fonctionnerait pas.

---

## üîí 3.5 Network Policies : la s√©curit√© r√©seau

Par d√©faut, **tous les pods peuvent communiquer entre eux** dans Kubernetes.
Mais dans un environnement de production, on veut souvent **restreindre la communication** (principe du moindre privil√®ge).

Les **NetworkPolicies** permettent de d√©finir des r√®gles :

* quels pods peuvent parler √† quels autres ;
* sur quels ports et protocoles.

Exemple :

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-web-to-db
spec:
  podSelector:
    matchLabels:
      app: database
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: webapp
    ports:
    - protocol: TCP
      port: 5432
```

‚Üí Seuls les pods `webapp` peuvent acc√©der aux pods `database` sur le port 5432.

> ‚ö†Ô∏è Toutes les CNIs ne supportent pas les NetworkPolicies (Cilium et Calico oui, Flannel non).

---

## üöÄ 4. Cilium : la nouvelle g√©n√©ration r√©seau

**Cilium** est un plugin CNI moderne bas√© sur la technologie **eBPF** (Extended Berkeley Packet Filter), int√©gr√©e au noyau Linux.

### Avantages :

* Performances tr√®s √©lev√©es (pas de tables iptables massives).
* Observabilit√© native : m√©triques, traces et visualisation des flux.
* S√©curit√© granulaire : politiques bas√©es sur les identit√©s et les services.
* Support du chiffrement transparent des pods.

> üîç Cilium transforme Kubernetes en une plateforme r√©seau programmable et observable.

---

## üï∏Ô∏è 5. Communication inter-n≈ìuds

Lorsqu‚Äôun pod sur un n≈ìud A doit parler √† un pod sur un n≈ìud B :

1. Le plugin CNI g√®re la **routabilit√© inter-n≈ìuds** (via tunnels, routage IP direct ou VXLAN).
2. kube-proxy s‚Äôassure que le service pointe vers les bons endpoints.
3. Le trafic passe soit directement (routage IP), soit encapsul√© (overlay).

Le choix du plugin d√©termine cette topologie :

* **Flannel** : overlay VXLAN (tunnels virtuels).
* **Calico / Cilium** : routage IP direct (plus efficace).

---

## üìä 6. Sch√©ma r√©capitulatif

```
                  +-----------------------------+
                  |       CONTROL PLANE         |
                  | (API Server, etcd, DNS info)|
                  +-------------+---------------+
                                |
                (watch/update via API)
                                |
+---------------------+   +---------------------+
|     WORKER NODE 1   |   |     WORKER NODE 2   |
|---------------------|   |---------------------|
| kubelet  kube-proxy |   | kubelet  kube-proxy |
| CNI (Cilium/Calico) |   | CNI (Cilium/Calico) |
|                     |   |                     |
| PodA  PodB          |   | PodC  PodD          |
+---------------------+   +---------------------+
        ‚Üï                          ‚Üï
     Communication inter-n≈ìuds via CNI
```

---

## ‚úÖ 7. Conclusion

Le r√©seau Kubernetes est un **√©cosyst√®me dynamique et modulaire**.
Chaque composant a un r√¥le pr√©cis :

* le **CNI** connecte les pods,
* le **kube-proxy** dirige le trafic,
* le **CoreDNS** assure la d√©couverte des services,
* les **NetworkPolicies** s√©curisent,
* et des solutions modernes comme **Cilium** enrichissent le tout par la performance et la visibilit√©.

Comprendre cette pile, c‚Äôest comprendre **comment Kubernetes relie tout ce qui vit dans ton cluster** ‚Äî et c‚Äôest aussi la cl√© pour diagnostiquer, s√©curiser et faire √©voluer tes applications en production.
